#!/usr/bin/env python3
"""Comprehensive testing and quality gates runner."""

import asyncio
import logging
import subprocess
import sys
import time
from pathlib import Path
from typing import Dict, List, Any

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class QualityGateRunner:
    """Runs comprehensive quality gates and tests."""
    
    def __init__(self):
        """Initialize quality gate runner."""
        self.results: Dict[str, Any] = {}
        self.passed_gates = 0
        self.total_gates = 0
        
    async def run_all_gates(self) -> bool:
        """Run all quality gates.
        
        Returns:
            True if all gates pass
        """
        logger.info("üöÄ Starting Comprehensive Quality Gates")
        start_time = time.time()
        
        # Define quality gates
        gates = [
            ("Security Validation", self.run_security_tests),
            ("Unit Tests", self.run_unit_tests),
            ("Integration Tests", self.run_integration_tests),
            ("Performance Tests", self.run_performance_tests),
            ("Code Quality", self.run_code_quality_checks),
            ("Documentation", self.run_documentation_checks),
            ("Robustness Tests", self.run_robustness_tests),
            ("Scalability Tests", self.run_scalability_tests),
        ]
        
        self.total_gates = len(gates)
        all_passed = True
        
        for gate_name, gate_function in gates:
            logger.info(f"\n{'='*60}")
            logger.info(f"üîç Running {gate_name}")
            logger.info(f"{'='*60}")
            
            try:
                result = await gate_function()
                self.results[gate_name] = result
                
                if result.get("passed", False):
                    self.passed_gates += 1
                    logger.info(f"‚úÖ {gate_name}: PASSED")
                else:
                    all_passed = False
                    logger.error(f"‚ùå {gate_name}: FAILED")
                    if "errors" in result:
                        for error in result["errors"]:
                            logger.error(f"   - {error}")
                            
            except Exception as e:
                all_passed = False
                logger.error(f"‚ùå {gate_name}: EXCEPTION - {e}")
                self.results[gate_name] = {"passed": False, "error": str(e)}
        
        # Generate final report
        total_time = time.time() - start_time
        await self.generate_final_report(total_time, all_passed)
        
        return all_passed
    
    async def run_security_tests(self) -> Dict[str, Any]:
        """Run security validation tests."""
        results = {"passed": True, "tests": [], "errors": []}
        
        try:
            # Test input validation
            logger.info("Testing input validation...")
            from llm_tab_cleaner.validation import InputValidator
            import pandas as pd
            
            validator = InputValidator()
            
            # Test malicious DataFrame
            malicious_data = {
                'column": DROP TABLE users; --': ['safe_value'],
                'normal_column': ['<script>alert("xss")</script>']
            }
            
            try:
                df = pd.DataFrame(malicious_data)
                result = validator.validate_dataframe(df)
                if not result.warnings:
                    results["errors"].append("Failed to detect malicious column names")
                    results["passed"] = False
                else:
                    results["tests"].append("‚úÖ Malicious input detection works")
            except Exception as e:
                results["tests"].append(f"‚úÖ DataFrame creation blocked malicious input: {e}")
            
            # Test file path traversal
            logger.info("Testing file path security...")
            path_result = validator.validate_file_path("../../etc/passwd")
            if path_result.is_valid:
                results["errors"].append("Failed to block path traversal attempt")
                results["passed"] = False
            else:
                results["tests"].append("‚úÖ Path traversal detection works")
            
            # Test data sanitization
            logger.info("Testing data sanitization...")
            from llm_tab_cleaner.validation import DataSanitizer
            sanitizer = DataSanitizer()
            
            malicious_df = pd.DataFrame({
                'test': ['<script>evil()</script>', 'normal_data', 'DROP TABLE test;']
            })
            
            sanitized_df, warnings = sanitizer.sanitize_dataframe(malicious_df)
            if warnings:
                results["tests"].append("‚úÖ Data sanitization works")
            else:
                results["errors"].append("Data sanitization did not trigger warnings")
                results["passed"] = False
                
        except Exception as e:
            results["errors"].append(f"Security test exception: {e}")
            results["passed"] = False
        
        return results
    
    async def run_unit_tests(self) -> Dict[str, Any]:
        """Run unit tests."""
        results = {"passed": True, "coverage": 0, "tests_run": 0, "errors": []}
        
        try:
            # Test core functionality
            logger.info("Testing core cleaning functionality...")
            from llm_tab_cleaner import TableCleaner
            import pandas as pd
            import numpy as np
            
            # Create test data
            test_data = pd.DataFrame({
                'id': [1, 2, 3, 4, 5],
                'name': ['John', 'N/A', 'Jane', '', 'Bob'],
                'age': [25, -1, 30, 999, 35],
                'email': ['john@test.com', 'invalid-email', 'jane@test.com', 'N/A', 'bob@test.com']
            })
            
            # Test basic cleaning
            cleaner = TableCleaner(
                llm_provider="local",
                confidence_threshold=0.7,
                enable_security=False,  # Disable for faster testing
                enable_backup=False
            )
            
            cleaned_df, report = cleaner.clean(test_data, sample_rate=0.5)
            
            if report.total_fixes >= 0:  # Should have some fixes
                results["tests_run"] += 1
                logger.info(f"‚úÖ Basic cleaning test: {report.total_fixes} fixes applied")
            else:
                results["errors"].append("Basic cleaning returned negative fixes")
                results["passed"] = False
            
            # Test adaptive features
            logger.info("Testing adaptive features...")
            from llm_tab_cleaner.adaptive import AdaptiveCache, PatternLearner
            
            cache = AdaptiveCache(max_size=100)
            cache.put("test", "column", {"data_type": "string"}, "cleaned_value", 0.9)
            cached_result = cache.get("test", "column", {"data_type": "string"})
            
            if cached_result and cached_result[0] == "cleaned_value":
                results["tests_run"] += 1
                logger.info("‚úÖ Adaptive cache test passed")
            else:
                results["errors"].append("Adaptive cache test failed")
                results["passed"] = False
            
            # Test streaming features
            logger.info("Testing streaming features...")
            from llm_tab_cleaner.streaming import StreamRecord
            
            record = StreamRecord(
                id="test_record",
                data={"name": "Test", "value": 42},
                timestamp=time.time()
            )
            
            if record.id == "test_record":
                results["tests_run"] += 1
                logger.info("‚úÖ Streaming record test passed")
            else:
                results["errors"].append("Streaming record test failed")
                results["passed"] = False
                
        except Exception as e:
            results["errors"].append(f"Unit test exception: {e}")
            results["passed"] = False
        
        results["coverage"] = min(90, results["tests_run"] * 30)  # Estimate coverage
        return results
    
    async def run_integration_tests(self) -> Dict[str, Any]:
        """Run integration tests."""
        results = {"passed": True, "tests": [], "errors": []}
        
        try:
            # Test end-to-end cleaning pipeline
            logger.info("Testing end-to-end pipeline...")
            from llm_tab_cleaner import TableCleaner
            from llm_tab_cleaner.distributed import DistributedCleaner
            import pandas as pd
            
            # Create larger test dataset
            import numpy as np
            np.random.seed(42)
            test_data = pd.DataFrame({
                'id': range(1000),
                'category': np.random.choice(['A', 'B', 'C', 'N/A', ''], size=1000),
                'value': np.random.choice([1, 2, 3, -1, 999, None], size=1000),
                'description': np.random.choice(['Good', 'Bad', 'N/A', '', 'Unknown'], size=1000)
            })
            
            # Test distributed processing
            base_config = {
                'llm_provider': 'local',
                'confidence_threshold': 0.8,
                'enable_security': False,
                'enable_backup': False
            }
            
            distributed_cleaner = DistributedCleaner(
                base_cleaner_config=base_config,
                max_workers=2,
                chunk_size=200,
                enable_process_pool=False  # Use threads for testing
            )
            
            report = distributed_cleaner.clean_distributed(
                test_data,
                sample_rate=0.1  # Process smaller sample
            )
            
            if report.processing_time > 0:
                results["tests"].append("‚úÖ Distributed processing works")
                logger.info(f"Distributed processing: {report.processing_time:.2f}s")
            else:
                results["errors"].append("Distributed processing failed")
                results["passed"] = False
            
            # Test monitoring integration
            logger.info("Testing monitoring integration...")
            from llm_tab_cleaner.optimization import get_global_resource_monitor
            
            monitor = get_global_resource_monitor()
            metrics = monitor.get_current_metrics()
            
            if metrics and metrics.cpu_percent >= 0:
                results["tests"].append("‚úÖ Resource monitoring works")
            else:
                results["errors"].append("Resource monitoring failed")
                results["passed"] = False
                
        except Exception as e:
            results["errors"].append(f"Integration test exception: {e}")
            results["passed"] = False
        
        return results
    
    async def run_performance_tests(self) -> Dict[str, Any]:
        """Run performance tests."""
        results = {"passed": True, "benchmarks": [], "errors": []}
        
        try:
            # Test processing speed
            logger.info("Testing processing speed...")
            from llm_tab_cleaner import TableCleaner
            import pandas as pd
            import time
            
            # Create performance test data
            test_sizes = [100, 500, 1000]
            
            for size in test_sizes:
                test_data = pd.DataFrame({
                    'id': range(size),
                    'data': [f"value_{i}" if i % 10 != 0 else "N/A" for i in range(size)]
                })
                
                cleaner = TableCleaner(
                    llm_provider="local",
                    enable_security=False,
                    enable_backup=False
                )
                
                start_time = time.time()
                _, report = cleaner.clean(test_data, sample_rate=0.1)
                processing_time = time.time() - start_time
                
                throughput = size / processing_time if processing_time > 0 else 0
                
                results["benchmarks"].append({
                    "size": size,
                    "processing_time": processing_time,
                    "throughput": throughput,
                    "fixes": report.total_fixes
                })
                
                logger.info(f"Size {size}: {processing_time:.3f}s ({throughput:.1f} records/sec)")
            
            # Check performance targets
            avg_throughput = sum(b["throughput"] for b in results["benchmarks"]) / len(results["benchmarks"])
            
            if avg_throughput > 100:  # Target: > 100 records/sec
                results["performance_grade"] = "A"
            elif avg_throughput > 50:
                results["performance_grade"] = "B"
            else:
                results["performance_grade"] = "C"
                results["errors"].append(f"Low throughput: {avg_throughput:.1f} records/sec")
                results["passed"] = False
            
            # Test memory usage
            logger.info("Testing memory efficiency...")
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            memory_before = process.memory_info().rss / 1024 / 1024  # MB
            
            # Create and clean large dataset
            large_data = pd.DataFrame({
                'col1': range(5000),
                'col2': [f"data_{i}" for i in range(5000)]
            })
            
            cleaner = TableCleaner(llm_provider="local", enable_backup=False)
            _, _ = cleaner.clean(large_data, sample_rate=0.05)
            
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = memory_after - memory_before
            
            if memory_increase < 100:  # Less than 100MB increase
                results["memory_grade"] = "Good"
            else:
                results["memory_grade"] = "Needs improvement"
                results["errors"].append(f"High memory usage: {memory_increase:.1f}MB increase")
                
        except Exception as e:
            results["errors"].append(f"Performance test exception: {e}")
            results["passed"] = False
        
        return results
    
    async def run_code_quality_checks(self) -> Dict[str, Any]:
        """Run code quality checks."""
        results = {"passed": True, "checks": [], "errors": []}
        
        try:
            # Check import structure
            logger.info("Checking import structure...")
            try:
                import llm_tab_cleaner
                version_info = llm_tab_cleaner.get_version_info()
                
                if version_info["features"]["core_cleaning"]:
                    results["checks"].append("‚úÖ Core imports work")
                else:
                    results["errors"].append("Core cleaning feature not available")
                    results["passed"] = False
                    
            except ImportError as e:
                results["errors"].append(f"Import error: {e}")
                results["passed"] = False
            
            # Check module structure
            logger.info("Checking module structure...")
            expected_modules = [
                'core', 'profiler', 'llm_providers', 'monitoring',
                'adaptive', 'streaming', 'distributed', 'caching',
                'optimization', 'validation', 'backup', 'health'
            ]
            
            missing_modules = []
            for module_name in expected_modules:
                try:
                    exec(f"from llm_tab_cleaner import {module_name}")
                except ImportError:
                    missing_modules.append(module_name)
            
            if missing_modules:
                results["errors"].append(f"Missing modules: {missing_modules}")
                results["passed"] = False
            else:
                results["checks"].append("‚úÖ All expected modules present")
            
            # Check error handling
            logger.info("Checking error handling...")
            from llm_tab_cleaner import TableCleaner
            import pandas as pd
            
            # Test with invalid data
            try:
                empty_df = pd.DataFrame()
                cleaner = TableCleaner(enable_backup=False, enable_security=False)
                
                try:
                    _, report = cleaner.clean(empty_df)
                    if report.total_fixes == 0:
                        results["checks"].append("‚úÖ Empty DataFrame handled correctly")
                    else:
                        results["errors"].append("Empty DataFrame not handled correctly") 
                        results["passed"] = False
                except ValueError as ve:
                    # Expected behavior - empty DataFrame should raise ValueError
                    if "empty" in str(ve).lower():
                        results["checks"].append("‚úÖ Empty DataFrame correctly rejected")
                    else:
                        results["errors"].append(f"Unexpected validation error: {ve}")
                        results["passed"] = False
                    
            except Exception as e:
                results["errors"].append(f"Error handling test failed: {e}")
                results["passed"] = False
                
        except Exception as e:
            results["errors"].append(f"Code quality check exception: {e}")
            results["passed"] = False
        
        return results
    
    async def run_documentation_checks(self) -> Dict[str, Any]:
        """Run documentation checks."""
        results = {"passed": True, "docs_found": [], "errors": []}
        
        try:
            # Check for required documentation files
            required_docs = [
                "README.md", "API_REFERENCE.md", "ARCHITECTURE.md",
                "DEPLOYMENT_GUIDE.md", "CONTRIBUTING.md"
            ]
            
            repo_root = Path("/root/repo")
            for doc in required_docs:
                doc_path = repo_root / doc
                if doc_path.exists():
                    results["docs_found"].append(f"‚úÖ {doc}")
                else:
                    results["errors"].append(f"Missing documentation: {doc}")
                    results["passed"] = False
            
            # Check docstring coverage
            logger.info("Checking docstring coverage...")
            from llm_tab_cleaner import TableCleaner
            
            if TableCleaner.__doc__:
                results["docs_found"].append("‚úÖ Core classes have docstrings")
            else:
                results["errors"].append("Missing docstrings in core classes")
                results["passed"] = False
            
            # Check examples directory
            examples_dir = repo_root / "examples"
            if examples_dir.exists():
                example_files = list(examples_dir.glob("*.py"))
                if example_files:
                    results["docs_found"].append(f"‚úÖ {len(example_files)} example files found")
                else:
                    results["errors"].append("No example files found")
                    results["passed"] = False
            else:
                results["errors"].append("Examples directory missing")
                results["passed"] = False
                
        except Exception as e:
            results["errors"].append(f"Documentation check exception: {e}")
            results["passed"] = False
        
        return results
    
    async def run_robustness_tests(self) -> Dict[str, Any]:
        """Run robustness tests."""
        results = {"passed": True, "tests": [], "errors": []}
        
        try:
            # Test with various edge cases
            logger.info("Testing robustness with edge cases...")
            from llm_tab_cleaner import TableCleaner
            import pandas as pd
            import numpy as np
            
            cleaner = TableCleaner(enable_backup=False, enable_security=False)
            
            # Edge case: All null values
            null_df = pd.DataFrame({
                'col1': [None, None, None],
                'col2': [np.nan, np.nan, np.nan]
            })
            
            try:
                _, report = cleaner.clean(null_df)
                results["tests"].append("‚úÖ All-null DataFrame handled")
            except Exception as e:
                results["errors"].append(f"All-null test failed: {e}")
                results["passed"] = False
            
            # Edge case: Single row
            single_row_df = pd.DataFrame({'value': ['test']})
            
            try:
                _, report = cleaner.clean(single_row_df)
                results["tests"].append("‚úÖ Single row DataFrame handled")
            except Exception as e:
                results["errors"].append(f"Single row test failed: {e}")
                results["passed"] = False
            
            # Edge case: Very wide DataFrame
            wide_df = pd.DataFrame({f'col_{i}': [f'val_{i}'] for i in range(100)})
            
            try:
                _, report = cleaner.clean(wide_df, sample_rate=0.1)
                results["tests"].append("‚úÖ Wide DataFrame handled")
            except Exception as e:
                results["errors"].append(f"Wide DataFrame test failed: {e}")
                results["passed"] = False
            
            # Test error recovery
            logger.info("Testing error recovery...")
            from llm_tab_cleaner.core import CircuitBreaker
            
            cb = CircuitBreaker(failure_threshold=2, timeout=1)
            
            def failing_function():
                raise Exception("Test failure")
            
            # Trigger circuit breaker
            for _ in range(3):
                try:
                    cb.call(failing_function)
                except:
                    pass
            
            if cb.state == "open":
                results["tests"].append("‚úÖ Circuit breaker works")
            else:
                results["errors"].append("Circuit breaker did not open")
                results["passed"] = False
                
        except Exception as e:
            results["errors"].append(f"Robustness test exception: {e}")
            results["passed"] = False
        
        return results
    
    async def run_scalability_tests(self) -> Dict[str, Any]:
        """Run scalability tests."""
        results = {"passed": True, "scalability_metrics": [], "errors": []}
        
        try:
            # Test distributed processing scalability
            logger.info("Testing distributed scalability...")
            from llm_tab_cleaner.distributed import DistributedCleaner
            import pandas as pd
            
            # Test different worker counts
            worker_counts = [1, 2, 4]
            data_size = 1000
            
            test_data = pd.DataFrame({
                'id': range(data_size),
                'value': [f"val_{i}" if i % 5 != 0 else "N/A" for i in range(data_size)]
            })
            
            base_config = {
                'llm_provider': 'local',
                'enable_backup': False,
                'enable_security': False
            }
            
            for workers in worker_counts:
                try:
                    distributed_cleaner = DistributedCleaner(
                        base_cleaner_config=base_config,
                        max_workers=workers,
                        chunk_size=200,
                        enable_process_pool=False
                    )
                    
                    start_time = time.time()
                    report = distributed_cleaner.clean_distributed(
                        test_data, sample_rate=0.1
                    )
                    processing_time = time.time() - start_time
                    
                    results["scalability_metrics"].append({
                        "workers": workers,
                        "processing_time": processing_time,
                        "throughput": data_size / processing_time if processing_time > 0 else 0
                    })
                    
                except Exception as e:
                    results["errors"].append(f"Scalability test with {workers} workers failed: {e}")
                    results["passed"] = False
            
            # Test auto-scaling
            logger.info("Testing auto-scaling...")
            from llm_tab_cleaner.distributed import AutoScaler
            
            scaler = AutoScaler(min_workers=1, max_workers=4)
            
            # Test scale up recommendation
            high_load_metrics = {
                "cpu_utilization": 0.9,
                "memory_utilization": 0.8,
                "queue_length": 10
            }
            
            recommendation = scaler.recommend_scaling(high_load_metrics)
            
            if recommendation["action"] == "scale_up":
                results["scalability_metrics"].append("‚úÖ Auto-scaling scale-up works")
            else:
                results["errors"].append("Auto-scaling scale-up failed")
                results["passed"] = False
            
            # Test caching scalability
            logger.info("Testing cache scalability...")
            from llm_tab_cleaner.caching import MultiLevelCache
            
            cache = MultiLevelCache(
                l1_size=50,
                l2_size=100,
                enable_disk_cache=False
            )
            
            # Fill cache with test data
            for i in range(150):  # More than L1 + L2
                cache.put(f"key_{i}", f"value_{i}")
            
            stats = cache.get_stats()
            
            if stats["access_stats"]["total_misses"] == 0:  # All puts should work
                results["scalability_metrics"].append("‚úÖ Multi-level cache scaling works")
            else:
                results["errors"].append("Multi-level cache scaling issues")
                results["passed"] = False
                
        except Exception as e:
            results["errors"].append(f"Scalability test exception: {e}")
            results["passed"] = False
        
        return results
    
    async def generate_final_report(self, total_time: float, all_passed: bool):
        """Generate final quality gates report."""
        logger.info(f"\n{'='*80}")
        logger.info("üéØ FINAL QUALITY GATES REPORT")
        logger.info(f"{'='*80}")
        
        status_emoji = "‚úÖ" if all_passed else "‚ùå"
        status_text = "PASSED" if all_passed else "FAILED"
        
        logger.info(f"{status_emoji} Overall Status: {status_text}")
        logger.info(f"üìä Gates Passed: {self.passed_gates}/{self.total_gates}")
        logger.info(f"‚è±Ô∏è  Total Time: {total_time:.2f} seconds")
        logger.info(f"üéØ Success Rate: {(self.passed_gates/self.total_gates)*100:.1f}%")
        
        logger.info(f"\nüìã Detailed Results:")
        for gate_name, result in self.results.items():
            status = "‚úÖ PASS" if result.get("passed", False) else "‚ùå FAIL"
            logger.info(f"   {status} {gate_name}")
            
            if not result.get("passed", False) and "errors" in result:
                for error in result["errors"][:3]:  # Show first 3 errors
                    logger.info(f"      - {error}")
        
        if all_passed:
            logger.info(f"\nüéâ All quality gates passed! System is production ready.")
        else:
            logger.info(f"\n‚ö†Ô∏è  Some quality gates failed. Review errors above.")
        
        logger.info(f"{'='*80}")


async def main():
    """Main function to run quality gates."""
    runner = QualityGateRunner()
    success = await runner.run_all_gates()
    
    # Exit with appropriate code
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())